# Configuration for experiments
dims: 14
search_method: turbo
obj_func_name: ptycho
num_acquisitions: 100
num_samples_per_acquisition: 1
surrogate: null
num_init_samples: 20
mode: fast

# Hyper-parameters for ptychography
func_args:
  file_dir: data/MoS2_10layer_80kV_cutoff20_defocus130_nyquist60_abr_noise10000_v2.h5
  param_names: [semiangle_cutoff, energy, num_iter, step_size, num_slices, slice_thicknesses, defocus, C12, phi12, C30, C21, phi21, C23, phi23]
  lb: [ 1,    1e3,   1,   0.01,    1,    0.1,  -200,     0,     0,   -5e4,    0,      0,    0,     0]
  ub: [30,  200e3,  20,   1.00,   50,   10.0,   200,   100,  6.28,    5e4,  100,   6.28,  100,  6.28]

# Hyper-parameters for different DFO methods
search_method_args:
  turbo:
    n_trust_regions: 5        # Number trust regions: TuRBO-1 will be used if set to 1, otherwise TuRBOM will be used
    n_repeat: 1               # Number repeat time for the same condition
    batch_size: 1             # How large batch size TuRBO uses
    verbose: True             # Print information from each batch
    use_ard: True             # Set to true if you want to use ARD for the GP kernel 
    max_cholesky_size: 2000   # When we switch from Cholesky to Lanczos
    n_training_steps: 50      # Number of steps of ADAM to learn the hypers
    min_cuda: 1024            # Run on the CPU for small datasets
    device: cpu               # "cpu" or "cuda"
    dtype: float32            # float64 or float32

  lamcts:
    Cp: 1                     # Cp for MCTS
    leaf_size: 10             # Tree leaf size
    kernel_type: linear       # SVM configruation
    gamma_type: auto          # SVM configruation

  da:
    initial_temp: 0.05

  doo:
    explr_p: 0.01

  voo:
    explr_p: 0.001
    sampling_mode: centered_uniform
    switch_counter: 100